{
  "project": {
    "name": "MCP Server Test Suite Modernization",
    "description": "Migrate test suite from custom test runners to Node.js native test framework, fix brittle tests, strengthen assertions, and improve test data quality to achieve >80 test quality score"
  },
  "tasks": [
    {
      "id": "001",
      "title": "Migrate to Node.js native test framework (node:test)",
      "category": "infrastructure",
      "priority": "critical",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [],
      "tags": ["testing", "infrastructure", "migration", "node:test"],
      "estimatedTokens": 650,
      "estimatedHours": 8,
      "prompt": {
        "context": "MCP server test suite uses 13 custom test runner scripts without a standard framework. Tests lack parallel execution, code coverage, watch mode, filtering, and IDE integration. Current test quality score: 72/100. Tech stack: Node.js with native assert module. No external test frameworks currently installed.",
        "objective": "Migrate all 13 test files from custom test runners to Node.js's built-in node:test module while preserving existing test logic and maintaining 100% test compatibility.",
        "style": "Follow Node.js test module conventions. Use import/export syntax for ESM modules. Organize tests with describe() blocks for grouping. Use test() function for individual test cases. Maintain existing assertion patterns with node:assert. Production-ready migration - zero regression tolerance.",
        "tone": "Critical priority - blocks all other test improvements. Conservative approach - preserve all existing test coverage. High urgency - foundational change enabling subsequent quality improvements. Prefer incremental migration over big-bang approach.",
        "audience": "Senior Node.js engineer familiar with test frameworks, module systems (CommonJS/ESM), async/await patterns, and test migration strategies. Experienced with node:test API and test lifecycle hooks.",
        "response": "Deliverables:\n1. package.json updated with test script using node --test\n2. All 13 test files migrated to node:test format\n3. Migration guide documenting pattern changes\n4. Validation script confirming all tests pass\n\nConstraints:\n- Node.js 18+ required (node:test availability)\n- Zero test coverage loss during migration\n- Preserve existing test names and structure\n- Maintain backwards compatibility with CI/CD\n- Use ESM modules (import/export) not CommonJS\n\nAcceptance Criteria:\n- [ ] All 13 test files use node:test API (test(), describe())\n- [ ] npm test runs all tests with node --test command\n- [ ] All existing test assertions preserved and passing\n- [ ] Test output includes pass/fail counts and durations\n- [ ] No custom test runner code remains\n- [ ] Tests can run individually with --test-name-pattern flag\n\nOut of Scope:\n- Code coverage reporting (separate task)\n- Test parallelization configuration\n- Watch mode setup\n- Performance optimization"
      }
    },
    {
      "id": "002",
      "title": "Fix async subprocess tests with event-based waiting",
      "category": "backend",
      "priority": "critical",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "async", "subprocess", "reliability"],
      "estimatedTokens": 550,
      "estimatedHours": 6,
      "prompt": {
        "context": "test-mcp-stats.js and test-real-compressions.js spawn MCP server subprocesses and use fixed 2-second timeouts for responses. This causes flaky tests - timeouts on slow systems or premature success on fast systems. Current code uses setTimeout() without listening for actual completion events. Tests interact via JSON-RPC protocol over stdin/stdout.",
        "objective": "Replace all fixed setTimeout calls with event-based waiting that listens for actual response messages from spawned subprocesses, implementing proper JSON-RPC message parsing and completion detection.",
        "style": "Use Node.js child_process events (stdout.on('data'), stderr.on('data'), on('close')). Implement streaming JSON-RPC parser with line buffering. Follow async/await patterns with Promise wrappers. Add configurable timeout as fallback (default 10s). Production-grade error handling with specific error types.",
        "tone": "High priority - eliminates major source of test flakiness. Pragmatic approach - use native Node.js APIs, no external libraries unless necessary. Medium risk tolerance - careful testing required for subprocess communication.",
        "audience": "Mid-level backend engineer familiar with Node.js child_process API, streams, JSON-RPC protocol, and async patterns. Understanding of process communication via stdin/stdout.",
        "response": "Deliverables:\n1. Refactored callMCPTool() function with event-based waiting\n2. JSON-RPC message parser with line buffering\n3. Helper functions for subprocess lifecycle management\n4. Updated test-mcp-stats.js and test-real-compressions.js\n\nConstraints:\n- No external process management libraries\n- Must handle partial JSON messages (line buffering)\n- Configurable timeout with clear error messages\n- Proper cleanup of spawned processes\n- Support both single and multi-line JSON-RPC responses\n\nAcceptance Criteria:\n- [ ] Zero setTimeout() calls for subprocess waiting\n- [ ] Event listeners on stdout for response detection\n- [ ] JSON-RPC parser handles multi-line responses\n- [ ] Timeout fallback with descriptive error (default 10s)\n- [ ] Process cleanup on test completion/failure\n- [ ] Tests pass reliably 10/10 times on CI\n- [ ] Test execution time reduced by >30%\n\nOut of Scope:\n- JSON-RPC client library implementation\n- Retry logic for failed requests\n- Performance benchmarking"
      }
    },
    {
      "id": "003",
      "title": "Implement robust JSON-RPC client for test communication",
      "category": "backend",
      "priority": "high",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "002", "type": "finish-to-start"}
      ],
      "tags": ["testing", "json-rpc", "communication", "reliability"],
      "estimatedTokens": 480,
      "estimatedHours": 5,
      "prompt": {
        "context": "test-real-compressions.js contains fragile JSON-RPC parsing logic that loops backwards through lines, tries JSON.parse, and silently ignores errors. This brittle approach assumes specific output format and lacks proper message framing. Current implementation in lines 75-148 is unmaintainable and error-prone.",
        "objective": "Create a reusable JSON-RPC test client module with proper message framing, request/response correlation, error handling, and timeout management for robust subprocess communication.",
        "style": "Module-based architecture - create lib/test-jsonrpc-client.js. Implement JSON-RPC 2.0 spec compliance (id, method, params, result/error). Use async/await with proper error propagation. Add TypeScript-style JSDoc for IDE support. Follow Node.js best practices for stream processing.",
        "tone": "High priority - replaces critical anti-pattern. Library-first approach - reusable across all subprocess tests. Medium urgency - improvement over task 002. Conservative - implement only needed JSON-RPC subset.",
        "audience": "Senior backend engineer familiar with JSON-RPC 2.0 specification, Node.js streams, protocol design, and test utility development. Understanding of request correlation and message framing.",
        "response": "Deliverables:\n1. lib/test-jsonrpc-client.js module with JSONRPCClient class\n2. Methods: call(method, params), initialize(), close()\n3. Request/response correlation by ID\n4. Integration into test-real-compressions.js\n5. Unit tests for JSON-RPC client\n\nConstraints:\n- JSON-RPC 2.0 spec compliance\n- Support both notification and request/response patterns\n- Handle out-of-order responses\n- Proper error object parsing {code, message, data}\n- Timeout per request (configurable)\n\nAcceptance Criteria:\n- [ ] JSONRPCClient class with call() and initialize() methods\n- [ ] Request ID auto-increment and correlation\n- [ ] Handles JSON-RPC error responses properly\n- [ ] Timeout throws specific JSONRPCTimeoutError\n- [ ] Backwards loop parsing removed from tests\n- [ ] All subprocess tests use new client\n- [ ] Client has >90% test coverage\n\nOut of Scope:\n- Batch request support\n- WebSocket transport\n- JSON-RPC 1.0 compatibility"
      }
    },
    {
      "id": "004",
      "title": "Add try/finally blocks for test data backup/restore",
      "category": "backend",
      "priority": "critical",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "data-safety", "cleanup", "reliability"],
      "estimatedTokens": 380,
      "estimatedHours": 3,
      "prompt": {
        "context": "test-real-compressions.js backs up user stats files before tests but manual restore pattern is error-prone. If tests crash or fail before restore, user data remains in backed-up state or gets lost. Current backup/restore functions in lines 37-57 and 328-346 lack guaranteed cleanup. Production user data at risk.",
        "objective": "Wrap all test execution in try/finally blocks that guarantee stats file restoration even on test failure, timeout, or crash, protecting user data integrity.",
        "style": "Use try/finally pattern consistently. Implement test fixtures with beforeEach/afterEach hooks (node:test). Add verification that backup exists before restore. Include detailed error logging for backup/restore failures. Follow fail-safe design principles.",
        "tone": "Critical priority - user data protection is non-negotiable. Conservative approach - multiple safety checks. Zero tolerance for data loss. High urgency - current implementation is unsafe.",
        "audience": "Mid-level backend engineer familiar with error handling, file system operations, test lifecycle hooks, and fail-safe design patterns. Understanding of process.on('uncaughtException') for crash handling.",
        "response": "Deliverables:\n1. Refactored test-real-compressions.js with try/finally protection\n2. beforeEach/afterEach hooks for backup/restore\n3. Verification function checking backup existence\n4. Error logging for backup/restore operations\n5. Documentation on safe test data handling\n\nConstraints:\n- Restore must execute even on test timeout\n- Handle ENOENT errors gracefully (file doesn't exist)\n- Verify backup file exists before attempting restore\n- Log all backup/restore operations with timestamps\n- Support nested test suites with proper cleanup\n\nAcceptance Criteria:\n- [ ] All test execution wrapped in try/finally blocks\n- [ ] beforeEach creates backup, afterEach restores\n- [ ] Backup verification before restore attempt\n- [ ] Tests artificially crashed still restore data\n- [ ] Error logs include file paths and timestamps\n- [ ] No manual backupStats()/restoreStats() calls remain\n- [ ] Manual test: kill -9 during test execution, verify restore\n\nOut of Scope:\n- Backup versioning\n- Incremental backups\n- Cloud backup integration"
      }
    },
    {
      "id": "005",
      "title": "Import actual server functions instead of duplicating code",
      "category": "backend",
      "priority": "high",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "maintainability", "code-quality", "refactoring"],
      "estimatedTokens": 420,
      "estimatedHours": 4,
      "prompt": {
        "context": "test-stats-retention.js duplicates aggregateStats() and migrateStatsFormat() functions from server.js (lines 158-177). Test passes with duplicated logic but doesn't catch bugs in actual production code. Code duplication violates DRY principle and creates maintenance burden. Server.js exports may need refactoring for testability.",
        "objective": "Refactor server.js to export testable functions and update test-stats-retention.js to import actual production code, eliminating code duplication while maintaining test coverage.",
        "style": "Export functions explicitly from server.js using module.exports or export statements. Use named exports for better tree-shaking. Add JSDoc comments documenting exported functions. Maintain existing function signatures for backwards compatibility. Follow Node.js module best practices.",
        "tone": "High priority - prevents false confidence in tests. Pragmatic approach - minimal refactoring to enable testing. Standard risk tolerance - well-understood pattern. Medium urgency - quality improvement.",
        "audience": "Mid-level backend engineer familiar with Node.js module systems, export/import patterns, test-driven development, and refactoring techniques. Understanding of coupling vs cohesion tradeoffs.",
        "response": "Deliverables:\n1. server.js refactored with explicit exports\n2. test-stats-retention.js importing actual functions\n3. Removed duplicate function implementations\n4. Updated JSDoc comments for exported functions\n5. Verification that tests still pass\n\nConstraints:\n- Zero breaking changes to server.js API\n- Maintain existing function signatures\n- Export only functions needed for testing\n- No circular dependencies introduced\n- CommonJS or ESM modules (match project standard)\n\nAcceptance Criteria:\n- [ ] server.js exports aggregateStats and migrateStatsFormat\n- [ ] test-stats-retention.js imports from server.js\n- [ ] Zero duplicate function implementations remain\n- [ ] All tests pass after refactoring\n- [ ] No new module dependencies added\n- [ ] JSDoc comments document exported functions\n- [ ] Code diff shows ~50 lines removed (duplicates)\n\nOut of Scope:\n- Full server.js modularization\n- Dependency injection implementation\n- Test doubles/mocks introduction"
      }
    },
    {
      "id": "006",
      "title": "Replace hardcoded dates with dynamic date generation",
      "category": "backend",
      "priority": "high",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "maintenance", "date-handling", "reliability"],
      "estimatedTokens": 350,
      "estimatedHours": 3,
      "prompt": {
        "context": "test-integration.js (lines 100-159) and test-date-parsing.js (lines 107-126) use hardcoded dates like '2025-01-01' that become stale over time. Tests checking for year === 2025 will fail in 2026+. Date assertions are brittle and require annual manual updates. No time-mocking library currently in use.",
        "objective": "Replace all hardcoded date assertions with dynamic relative date calculations that remain valid regardless of when tests run, eliminating time-based test failures.",
        "style": "Use Date.now() and relative calculations (e.g., new Date(Date.now() + 86400000)). Create helper functions for common date operations (daysFromNow, monthsAgo). Use ISO 8601 format for consistency. Add comments explaining relative date logic. Avoid external time-mocking libraries unless necessary.",
        "tone": "High priority - prevents future test failures. Pragmatic approach - use native Date API. Low risk tolerance - date bugs are subtle. Medium urgency - not currently broken but will be.",
        "audience": "Junior to mid-level engineer familiar with JavaScript Date API, ISO 8601 format, and test data generation. Basic understanding of timezone handling (UTC).",
        "response": "Deliverables:\n1. Date helper utilities (lib/test-date-helpers.js)\n2. Refactored test-integration.js with relative dates\n3. Refactored test-date-parsing.js with dynamic dates\n4. Documentation on date testing best practices\n\nConstraints:\n- Use native Date API (no external libraries)\n- All dates in UTC to avoid timezone issues\n- Maintain test readability with helper functions\n- Support relative dates (days/weeks/months/years ago/from now)\n- Preserve test intent and coverage\n\nAcceptance Criteria:\n- [ ] Zero hardcoded year values (2025, 2024, etc.)\n- [ ] Date helpers: daysAgo(), monthsAgo(), yearsAgo()\n- [ ] All date assertions use relative calculations\n- [ ] Tests pass regardless of execution date\n- [ ] Test execution from 2020-2030 verified\n- [ ] Comments explain relative date logic\n- [ ] ISO 8601 format used consistently\n\nOut of Scope:\n- Time-mocking library integration\n- Timezone conversion testing\n- Leap year edge case handling"
      }
    },
    {
      "id": "007",
      "title": "Convert manual test scenarios to automated tests",
      "category": "backend",
      "priority": "high",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"},
        {"taskId": "003", "type": "finish-to-start"}
      ],
      "tags": ["testing", "automation", "ci-cd", "coverage"],
      "estimatedTokens": 520,
      "estimatedHours": 6,
      "prompt": {
        "context": "test-stats-query.js (lines 218-262) documents test scenarios but prints 'Manual verification required' and marks them as passed without actual validation. Tests claim 100% pass rate but require human review. This reduces CI/CD reliability and creates false confidence. 13 documented scenarios need automation.",
        "objective": "Implement full automation for all 13 test scenarios in test-stats-query.js with proper assertions validating query results, error handling, and edge cases, eliminating manual verification requirement.",
        "style": "Use node:test framework with proper test() blocks. Implement assertions for each test case using assert module. Create test data fixtures for different date ranges. Add error case validation with specific error type checks. Follow arrange-act-assert pattern.",
        "tone": "High priority - fixes false positive tests. Standard approach - straightforward automation. Medium risk - careful validation needed. Medium urgency - improves CI/CD confidence.",
        "audience": "Mid-level backend engineer familiar with test automation, assertion libraries, data fixture creation, and MCP server query API. Understanding of date range filtering and pagination.",
        "response": "Deliverables:\n1. Fully automated test-stats-query.js with 13 test cases\n2. Test data fixtures for various date ranges\n3. Assertions validating query results\n4. Error case tests with specific error type checks\n5. Removed manual verification comments\n\nConstraints:\n- All 13 scenarios must be automated\n- Use actual MCP server for integration testing\n- Validate both success and error cases\n- Check result counts, date ranges, and pagination\n- Proper cleanup of test data after execution\n\nAcceptance Criteria:\n- [ ] Zero 'Manual verification required' messages\n- [ ] All 13 test scenarios have proper assertions\n- [ ] Tests validate query result counts\n- [ ] Date range filtering verified with assertions\n- [ ] Error cases throw specific error types\n- [ ] Tests fail when expected conditions not met\n- [ ] CI/CD runs tests without human intervention\n\nOut of Scope:\n- Performance testing of queries\n- Load testing with large datasets\n- Query optimization"
      }
    },
    {
      "id": "008",
      "title": "Replace console.log with proper assert statements",
      "category": "backend",
      "priority": "high",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "assertions", "quality", "reliability"],
      "estimatedTokens": 320,
      "estimatedHours": 3,
      "prompt": {
        "context": "test-schema-validation.js (lines 74-126) uses console.log for validation results instead of assertions. Tests always exit with code 0 even when validation fails. Five test files have weak assertions that don't fail test suite on errors. No actual test failures reported to CI/CD despite validation issues.",
        "objective": "Replace all console.log validation statements with proper assert.strictEqual() and assert.ok() calls that fail tests when validation doesn't pass, ensuring CI/CD catches real failures.",
        "style": "Use node:assert module strictEqual, ok, and deepStrictEqual methods. Remove all console.log validation statements. Throw AssertionError on failure with descriptive messages. Follow AAA pattern (Arrange-Act-Assert). Add assertion messages explaining expected vs actual.",
        "tone": "High priority - fixes core test infrastructure bug. Conservative approach - ensure all validations become assertions. Zero tolerance for false positives. High urgency - currently hiding failures.",
        "audience": "Junior to mid-level engineer familiar with assertion libraries, test-driven development, and Node.js assert module API. Understanding of test failure propagation.",
        "response": "Deliverables:\n1. Refactored test-schema-validation.js with assert statements\n2. Updated test-statistics.js with field validation assertions\n3. Refactored test-mcp-stats.js with schema validation\n4. Refactored test-cost-reporting.js with proper assertions\n5. Verification that tests actually fail when conditions not met\n\nConstraints:\n- Use node:assert module only (no external libraries)\n- All console.log('âœ“ ... PASS/FAIL') must be removed\n- Descriptive assertion messages\n- Tests must exit non-zero on validation failure\n- Maintain readability with clear assertion logic\n\nAcceptance Criteria:\n- [ ] Zero console.log validation statements remain\n- [ ] All validations use assert.strictEqual or assert.ok\n- [ ] Tests fail when validation fails (verified manually)\n- [ ] Assertion messages explain expected values\n- [ ] Five test files refactored (schema, statistics, mcp-stats, cost-reporting, cost-tracking)\n- [ ] CI/CD exits non-zero on test failure\n- [ ] Test output shows AssertionError details\n\nOut of Scope:\n- Custom assertion library development\n- Assertion message i18n\n- Visual test reporting"
      }
    },
    {
      "id": "009",
      "title": "Implement epsilon-based floating point comparison",
      "category": "backend",
      "priority": "medium",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "008", "type": "finish-to-start"}
      ],
      "tags": ["testing", "floating-point", "precision", "math"],
      "estimatedTokens": 280,
      "estimatedHours": 2,
      "prompt": {
        "context": "test-cost-reporting.js (lines 122-127) uses Math.round() hack for floating point comparison with 4 decimal places. This approach may hide precision bugs and isn't semantically correct. Tests involve currency calculations where precision matters. No proper floating point comparison utility exists.",
        "objective": "Create a floating point comparison utility with configurable epsilon tolerance and refactor tests to use semantically correct approximate equality checks instead of rounding hacks.",
        "style": "Create lib/test-math-helpers.js module. Implement assertAlmostEqual(actual, expected, epsilon, message) function. Use standard epsilon of 1e-10 for general comparisons, 0.01 for currency. Add JSDoc documentation. Follow IEEE 754 floating point best practices.",
        "tone": "Medium priority - improves test correctness. Pragmatic approach - solve specific problem without over-engineering. Standard risk - well-understood numerical computing. Low urgency - current implementation works but not ideal.",
        "audience": "Mid-level engineer familiar with floating point arithmetic, IEEE 754 standard, epsilon comparison, and currency handling. Understanding of numerical precision issues.",
        "response": "Deliverables:\n1. lib/test-math-helpers.js with assertAlmostEqual function\n2. Refactored test-cost-reporting.js using epsilon comparison\n3. Documentation on floating point testing best practices\n4. Unit tests for math helper functions\n\nConstraints:\n- Configurable epsilon per assertion\n- Default epsilon: 1e-10 for general, 0.01 for currency\n- Clear error messages showing actual vs expected\n- Handle NaN and Infinity edge cases\n- Support both positive and negative numbers\n\nAcceptance Criteria:\n- [ ] assertAlmostEqual function implemented\n- [ ] Math.round() hack removed from tests\n- [ ] Epsilon of 0.01 used for currency comparisons\n- [ ] Error messages show actual, expected, and epsilon\n- [ ] Edge cases tested (NaN, Infinity, -0)\n- [ ] All cost reporting tests pass\n- [ ] JSDoc documentation complete\n\nOut of Scope:\n- Complex number support\n- Vector/matrix comparisons\n- Arbitrary precision arithmetic"
      }
    },
    {
      "id": "010",
      "title": "Add comprehensive field validation to compression records",
      "category": "backend",
      "priority": "medium",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "008", "type": "finish-to-start"}
      ],
      "tags": ["testing", "validation", "data-integrity", "schema"],
      "estimatedTokens": 400,
      "estimatedHours": 4,
      "prompt": {
        "context": "test-statistics.js (lines 100-106) and test-mcp-stats.js (lines 79-97) only check if compression records exist, not that fields have correct values. Tests pass if file exists with any JSON structure. No validation of tokens, ratios, timestamps, or other critical fields. Schema violations could go undetected.",
        "objective": "Implement comprehensive schema validation for compression records checking all required fields, data types, value ranges, and relationships, ensuring data integrity.",
        "style": "Create validateCompressionRecord(record) function with detailed checks. Validate field presence, types (number, string, boolean), ranges (tokens > 0, ratio 0-1), and calculated relationships (tokensSaved = original - compressed). Use assert.deepStrictEqual for nested objects. Return descriptive validation errors.",
        "tone": "Medium priority - prevents data corruption bugs. Thorough approach - validate all fields. Standard risk - straightforward validation. Medium urgency - improves confidence in data integrity.",
        "audience": "Mid-level backend engineer familiar with data validation, schema design, JSON structures, and test data verification. Understanding of compression statistics domain.",
        "response": "Deliverables:\n1. lib/test-validators.js with validateCompressionRecord function\n2. Refactored test-statistics.js with full field validation\n3. Refactored test-mcp-stats.js with schema validation\n4. Schema documentation for compression record structure\n5. Unit tests for validator functions\n\nConstraints:\n- Validate all required fields: timestamp, path, originalTokens, compressedTokens, tokensSaved, compressionRatio, savingsPercentage, level, format\n- Check data types match schema\n- Validate ranges: tokens > 0, ratio 0-1, percentage 0-100\n- Verify calculations: tokensSaved = original - compressed\n- Clear error messages indicating which field failed\n\nAcceptance Criteria:\n- [ ] validateCompressionRecord function implemented\n- [ ] All 9 required fields validated\n- [ ] Type checking for each field\n- [ ] Range validation for numeric fields\n- [ ] Calculation verification (savings = original - compressed)\n- [ ] Tests fail with descriptive errors on invalid data\n- [ ] Both test files use validation function\n- [ ] Schema documentation added to docs/\n\nOut of Scope:\n- JSON Schema integration\n- Custom validation DSL\n- Runtime schema enforcement in production"
      }
    },
    {
      "id": "011",
      "title": "Refactor env cleanup to use test lifecycle hooks",
      "category": "backend",
      "priority": "medium",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "cleanup", "isolation", "lifecycle"],
      "estimatedTokens": 320,
      "estimatedHours": 3,
      "prompt": {
        "context": "test-integration.js (lines 62-68) has manual clearEnvVars() function that must be called explicitly. Easy to forget, causing test pollution between test runs. test-cost-tracking.js (lines 241-244) manually resets caches. No automatic cleanup guarantees. Tests can affect each other if cleanup forgotten.",
        "objective": "Migrate all manual cleanup functions to node:test beforeEach/afterEach hooks that automatically run before and after each test, ensuring proper test isolation without manual intervention.",
        "style": "Use node:test lifecycle hooks: beforeEach, afterEach. Create test fixtures encapsulating setup/cleanup. Save original state in beforeEach, restore in afterEach. Use try/finally within afterEach for guaranteed cleanup. Follow test isolation best practices.",
        "tone": "Medium priority - improves test reliability. Standard approach - use framework features. Low risk - well-understood pattern. Medium urgency - prevents occasional test pollution bugs.",
        "audience": "Mid-level engineer familiar with test lifecycle hooks, test isolation principles, and state management in tests. Understanding of beforeEach/afterEach patterns.",
        "response": "Deliverables:\n1. Refactored test-integration.js with beforeEach/afterEach hooks\n2. Refactored test-cost-tracking.js with hook-based cleanup\n3. Removed manual clearEnvVars() and cache reset calls\n4. Test fixtures documentation\n5. Verification that tests remain isolated\n\nConstraints:\n- Use node:test beforeEach/afterEach hooks\n- Save original state before modification\n- Restore state in afterEach with try/finally\n- Zero manual cleanup function calls in tests\n- Support nested describe blocks with proper scope\n\nAcceptance Criteria:\n- [ ] beforeEach hooks save original env vars and cache state\n- [ ] afterEach hooks restore original state\n- [ ] Zero manual clearEnvVars() calls in test code\n- [ ] Zero manual cache reset calls in test code\n- [ ] Tests pass when run individually and in suite\n- [ ] Test execution order doesn't affect results\n- [ ] Try/finally ensures cleanup even on test failure\n\nOut of Scope:\n- Global test fixtures\n- Test data factories\n- Snapshot testing"
      }
    },
    {
      "id": "012",
      "title": "Replace synthetic test data with real file compression",
      "category": "backend",
      "priority": "low",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "test-data", "realism", "integration"],
      "estimatedTokens": 450,
      "estimatedHours": 5,
      "prompt": {
        "context": "test-statistics.js (lines 16-32), test-integration.js (lines 372-421), test-cost-reporting.js (lines 45-109), and test-statistics-fallback.js (lines 147-168) use hardcoded synthetic test content. Tests don't validate behavior with actual project files. Edge cases in real files may not be covered. Seven test files affected.",
        "objective": "Replace all synthetic test content with real file compression using actual project files (server.js, test files), creating realistic test fixtures that validate real-world compression scenarios and edge cases.",
        "style": "Create fixtures/ directory with actual file samples. Use real project files for compression tests. Generate varied test data with different file sizes and complexities. Store compressed output as fixtures for regression testing. Follow fixture-based testing patterns.",
        "tone": "Low priority - improvement over current but not blocking. Pragmatic approach - use existing project files. Low risk - straightforward refactoring. Low urgency - tests currently work with synthetic data.",
        "audience": "Junior to mid-level engineer familiar with test fixtures, file I/O, and test data generation. Understanding of compression testing and fixture management.",
        "response": "Deliverables:\n1. fixtures/ directory with real file samples\n2. Fixture generation script for test data\n3. Refactored test-statistics.js using real files\n4. Refactored test-integration.js with real fixtures\n5. Refactored test-cost-reporting.js with real data\n6. Refactored test-statistics-fallback.js with temp files\n7. Documentation on fixture management\n\nConstraints:\n- Use actual project files (server.js, test-*.js)\n- Fixtures committed to git (for consistency)\n- Varied file sizes: small (<1KB), medium (1-10KB), large (>10KB)\n- Include edge cases: empty files, binary files, special characters\n- Fixture regeneration script for updates\n\nAcceptance Criteria:\n- [ ] fixtures/ directory with 10+ real file samples\n- [ ] Seven test files use real file compression\n- [ ] Zero hardcoded TEST_CONTENT strings\n- [ ] Tests use actual server.js as test subject\n- [ ] Edge cases covered: empty, large, special chars\n- [ ] Fixture regeneration script documented\n- [ ] All tests pass with real fixtures\n\nOut of Scope:\n- Property-based testing\n- Fuzzing\n- Performance benchmarking"
      }
    },
    {
      "id": "013",
      "title": "Test actual config file path resolution behavior",
      "category": "backend",
      "priority": "low",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "configuration", "path-resolution", "integration"],
      "estimatedTokens": 340,
      "estimatedHours": 3,
      "prompt": {
        "context": "test-llm-detection.js (lines 209-222) creates config file at TEST_CONFIG_FILE path separate from production path. Test validates config loading but doesn't verify actual path resolution used in production. test-statistics-fallback.js (line 27) mocks UCPL_STATS_FILE env var without verifying server actually checks it.",
        "objective": "Add integration tests verifying production config and stats file path resolution logic, ensuring tests validate actual path behavior used by running server.",
        "style": "Test both default paths and environment variable overrides. Use actual production paths in separate test suite. Create integration tests that spawn real server process. Verify file locations match expectations. Add path resolution unit tests.",
        "tone": "Low priority - tests currently work but don't verify production paths. Standard approach - integration testing. Low risk - path resolution is straightforward. Low urgency - nice-to-have confidence boost.",
        "audience": "Junior to mid-level engineer familiar with file path resolution, environment variables, and integration testing. Understanding of config file loading patterns.",
        "response": "Deliverables:\n1. New test-path-resolution.js integration test suite\n2. Tests for default config path (~/.ucpl/compress/config.json)\n3. Tests for UCPL_STATS_FILE env var override\n4. Tests for UCPL_CONFIG_FILE env var override\n5. Updated test-llm-detection.js with path verification\n6. Documentation on config file locations\n\nConstraints:\n- Test actual production paths\n- Test environment variable overrides\n- Use real server process for integration tests\n- Verify files created at expected locations\n- Clean up test files after execution\n\nAcceptance Criteria:\n- [ ] test-path-resolution.js created with 5+ tests\n- [ ] Default config path tested: ~/.ucpl/compress/config.json\n- [ ] UCPL_STATS_FILE env var override verified\n- [ ] UCPL_CONFIG_FILE env var override verified\n- [ ] Server process spawned to test actual behavior\n- [ ] File existence verified at expected paths\n- [ ] All test files cleaned up after execution\n\nOut of Scope:\n- Config file schema validation\n- Config migration logic\n- Multi-user config support"
      }
    },
    {
      "id": "014",
      "title": "Generate varied test data with realistic token distribution",
      "category": "backend",
      "priority": "low",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "012", "type": "finish-to-start"}
      ],
      "tags": ["testing", "test-data", "statistics", "realism"],
      "estimatedTokens": 380,
      "estimatedHours": 4,
      "prompt": {
        "context": "test-stats-retention.js (lines 196-264) uses createMockCompression() helper that creates identical compression records (10000 original, 2500 compressed). All records have same 75% compression ratio. Real compressions vary widely by file type, compression level, and content. Edge cases with low/high ratios not tested.",
        "objective": "Create realistic test data generator producing varied compression records with different token counts, compression ratios, file types, and timestamps, covering edge cases and realistic distributions.",
        "style": "Create lib/test-data-generators.js module. Implement generateCompressionRecord(options) with configurable ranges. Use statistical distributions for realistic variation. Include edge cases: nearly incompressible (90% ratio), highly compressible (10% ratio), empty files. Add seeded randomness for reproducibility.",
        "tone": "Low priority - nice-to-have improvement. Pragmatic approach - focus on realistic variation not perfection. Low risk - test data generation. Low urgency - current data works but lacks variety.",
        "audience": "Mid-level engineer familiar with test data generation, statistical distributions, and data modeling. Understanding of compression statistics and edge cases.",
        "response": "Deliverables:\n1. lib/test-data-generators.js with generateCompressionRecord\n2. Preset generators: lowCompression, highCompression, average\n3. Seeded random number generator for reproducibility\n4. Refactored test-stats-retention.js with varied data\n5. Documentation on test data generation patterns\n\nConstraints:\n- Configurable token ranges (100-1000000)\n- Compression ratio distribution: 10%-90%\n- Support different levels: minimal, signatures, full\n- Seeded randomness for reproducible tests\n- Edge cases: empty files, nearly incompressible\n\nAcceptance Criteria:\n- [ ] generateCompressionRecord() function implemented\n- [ ] Preset generators for common scenarios\n- [ ] Token counts vary between 100-1000000\n- [ ] Compression ratios vary between 10%-90%\n- [ ] Seeded random ensures reproducible tests\n- [ ] Edge cases included: 0 tokens, 99% ratio\n- [ ] test-stats-retention.js uses varied data\n- [ ] No identical hardcoded token counts remain\n\nOut of Scope:\n- Machine learning-based data generation\n- Real-world compression ratio analysis\n- Property-based testing framework"
      }
    },
    {
      "id": "015",
      "title": "Add code coverage reporting with c8",
      "category": "infrastructure",
      "priority": "medium",
      "status": "PENDING",
      "framework": "COSTAR",
      "dependencies": [
        {"taskId": "001", "type": "finish-to-start"}
      ],
      "tags": ["testing", "coverage", "quality", "metrics"],
      "estimatedTokens": 380,
      "estimatedHours": 4,
      "prompt": {
        "context": "Test suite has no code coverage reporting. Unknown which code paths are tested. Cannot track coverage improvements over time. Report recommends >80% coverage target. Node.js native test runner works with c8 coverage tool (no Istanbul needed). No coverage configuration currently exists.",
        "objective": "Integrate c8 code coverage tool with npm test command, generate HTML and terminal reports, and establish 80% coverage baseline with enforcement in CI/CD.",
        "style": "Use c8 (native V8 coverage) not Istanbul. Add npm script: 'npm run test:coverage'. Configure c8 via package.json or .c8rc. Generate both terminal summary and HTML reports. Set coverage thresholds: 80% lines, 80% functions, 75% branches. Follow industry standard coverage practices.",
        "tone": "Medium priority - enables quality tracking. Standard approach - use established tooling. Low risk - coverage reporting doesn't affect code. Medium urgency - enables data-driven improvements.",
        "audience": "Mid-level engineer familiar with code coverage concepts, npm scripts, and CI/CD configuration. Understanding of coverage metrics (lines, branches, functions).",
        "response": "Deliverables:\n1. c8 package installed as dev dependency\n2. npm scripts: test:coverage, test:coverage:report\n3. .c8rc.json configuration file\n4. Coverage thresholds enforced (80% lines, 80% functions, 75% branches)\n5. HTML report generated in coverage/ directory\n6. CI/CD integration with coverage enforcement\n7. Documentation on interpreting coverage reports\n\nConstraints:\n- Use c8 not Istanbul (V8 native coverage)\n- HTML report for detailed inspection\n- Terminal summary for quick feedback\n- Coverage reports in .gitignore\n- Fail CI if below thresholds\n\nAcceptance Criteria:\n- [ ] c8 installed and configured\n- [ ] npm run test:coverage executes tests with coverage\n- [ ] Terminal shows coverage summary\n- [ ] HTML report generated in coverage/\n- [ ] Thresholds enforced: 80% lines, 80% functions, 75% branches\n- [ ] CI fails if coverage below thresholds\n- [ ] coverage/ directory in .gitignore\n- [ ] Documentation explains coverage metrics\n\nOut of Scope:\n- Mutation testing\n- Coverage trending over time\n- SonarQube integration"
      }
    }
  ]
}
